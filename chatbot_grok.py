import os
import streamlit as st
import time
import base64
import uuid
import tempfile
import io
import numpy as np
from PIL import Image
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.chains.combine_documents import create_stuff_documents_chain
import requests

# í˜ì´ì§€ ì„¤ì • - ëª¨ë°”ì¼ í˜¸í™˜ì„± ê°œì„ 
st.set_page_config(
    page_title="PDF Chatbot",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë°˜ì‘í˜• ë””ìì¸ë§Œ ìœ ì§€í•˜ê³  ê¸°ì¡´ ìƒ‰ìƒ ë³µì›
st.markdown("""
<style>
    /* ëª¨ë°”ì¼ í™˜ê²½ì—ì„œì˜ ìŠ¤íƒ€ì¼ ì¡°ì • */
    @media (max-width: 768px) {
        .main .block-container {
            padding-top: 1rem;
            padding-left: 0.5rem;
            padding-right: 0.5rem;
        }
        .stSidebar {
            width: 100% !important;
        }
        iframe {
            width: 100% !important;
            height: 400px !important;
        }
        .pdf-container {
            width: 100% !important;
        }
    }
</style>
""", unsafe_allow_html=True)

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from langchain_community.vectorstores import FAISS
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_core.messages import HumanMessage, SystemMessage
    from langchain.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.chains.combine_documents import create_stuff_documents_chain
    
    # Grok API ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
    from langchain_core.language_models.chat_models import BaseChatModel
    from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
    from langchain_core.outputs import ChatGeneration, ChatResult
    from typing import Any, Dict, List, Mapping, Optional
    
    # OpenAI ì„ë² ë”© ëª¨ë¸ (Grokì€ ì„ë² ë”© APIê°€ ì—†ì–´ì„œ OpenAI ì‚¬ìš©)
    try:
        from langchain_openai import OpenAIEmbeddings
        has_openai = True
    except ImportError:
        has_openai = False
        st.warning("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„ë² ë”© ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
except ImportError as e:
    st.error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {str(e)}")
    st.warning("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
    st.code("pip install langchain langchain-community langchain-openai faiss-cpu streamlit requests", language="bash")
    st.stop()

# Grok API í´ë˜ìŠ¤ ì •ì˜
class GrokChatModel(BaseChatModel):
    api_key: str
    temperature: float = 0.7
    max_tokens: int = 1024
    
    @property
    def _llm_type(self) -> str:
        return "grok-chat"
    
    def _generate(
        self,
        messages: List[Any],
        stop: Optional[List[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any,
    ) -> ChatResult:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
        formatted_messages = []
        for message in messages:
            if isinstance(message, SystemMessage):
                formatted_messages.append({"role": "system", "content": message.content})
            elif isinstance(message, HumanMessage):
                formatted_messages.append({"role": "user", "content": message.content})
            elif isinstance(message, AIMessage):
                formatted_messages.append({"role": "assistant", "content": message.content})
        
        payload = {
            "messages": formatted_messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        
        if stop:
            payload["stop"] = stop
        
        try:
            response = requests.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            response_data = response.json()
            
            # ì‘ë‹µ ì²˜ë¦¬
            message_content = response_data["choices"][0]["message"]["content"]
            
            return ChatResult(
                generations=[
                    ChatGeneration(
                        message=AIMessage(content=message_content),
                        generation_info={"finish_reason": response_data["choices"][0].get("finish_reason")}
                    )
                ]
            )
        except Exception as e:
            raise ValueError(f"Grok API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    async def _agenerate(
        self,
        messages: List[Any],
        stop: Optional[List[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any,
    ) -> ChatResult:
        # ë¹„ë™ê¸° êµ¬í˜„ì€ ë™ê¸° ë©”ì„œë“œë¥¼ í˜¸ì¶œ
        return self._generate(messages, stop, run_manager, **kwargs)

if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()
    st.session_state.file_cache = {}

session_id = st.session_state.id
client = None

# í™˜ê²½ ì„¤ì •
GROK_API_KEY = os.environ.get("GROK_API_KEY", "xai-BZDTPNCSBayfE5PKA7dStIN0mK19IJtmpdiCch9aIWQFNZZKj6WZyBzM8ss9OBZBIPBzisnBDT5nQRgK")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")

def reset_chat():
    st.session_state.messages = []
    st.session_state.context = None

def display_pdf(file):
    """PDF íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ iframeìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ í¬ì¸í„° ìœ„ì¹˜ ì´ˆê¸°í™”
        file.seek(0)
        
        # íŒŒì¼ ë°ì´í„°ë¥¼ ì½ì–´ì˜´
        file_data = file.read()
        
        # base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ iframeìœ¼ë¡œ í‘œì‹œ
        base64_pdf = base64.b64encode(file_data).decode('utf-8')
        
        # ë°˜ì‘í˜• ë””ìì¸ ìœ ì§€í•˜ë©´ì„œ ê¸°ì¡´ ìŠ¤íƒ€ì¼ë¡œ ë³µì›
        pdf_display = f'''
            <div class="pdf-container">
                <iframe src="data:application/pdf;base64,{base64_pdf}" 
                        width="100%"
                        height="500px"
                        type="application/pdf">
                    <p>PDFë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. <a href="data:application/pdf;base64,{base64_pdf}" download="{file.name}">PDF ë‹¤ìš´ë¡œë“œ</a></p>
                </iframe>
            </div>
        '''
        st.markdown("### PDF ë¯¸ë¦¬ë³´ê¸°")
        st.markdown(pdf_display, unsafe_allow_html=True)
        
        # ì›ë³¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ
        file.seek(0)
        st.download_button(
            label="PDF ë‹¤ìš´ë¡œë“œ",
            data=file_data,
            file_name=file.name,
            mime="application/pdf"
        )
        
    except Exception as e:
        st.error(f"PDF í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    # íŒŒì¼ í¬ì¸í„° ìœ„ì¹˜ ë‹¤ì‹œ ì´ˆê¸°í™”
    file.seek(0)

# LLM ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_basic_llm():
    if "basic_llm" not in st.session_state:
        try:
            st.session_state.basic_llm = GrokChatModel(
                api_key=GROK_API_KEY,
                temperature=0.7,
                max_tokens=1024
            )
        except Exception as e:
            st.error(f"Grok API ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            return None
    
    return st.session_state.basic_llm

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜
def get_embeddings_model():
    if has_openai:
        # OpenAI ì„ë² ë”© ì‚¬ìš© (Grokì€ ì„ë² ë”© APIê°€ ì—†ìŒ)
        if not OPENAI_API_KEY:
            st.sidebar.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            return None
        return OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    else:
        st.error("ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return None

# ì‚¬ì´ë“œë°” êµ¬ì„± - ëª¨ë°”ì¼ í™˜ê²½ ì§€ì› ì¶”ê°€
with st.sidebar:
    st.header(f"Chatbot Options")
    
    # OpenAI API í‚¤ ì…ë ¥ (ì„ë² ë”©ìš©)
    openai_api_key = st.text_input("OpenAI API Key (ì„ë² ë”©ìš©)", value=OPENAI_API_KEY, type="password")
    if openai_api_key:
        os.environ["OPENAI_API_KEY"] = openai_api_key
        OPENAI_API_KEY = openai_api_key
    
    # ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
    if st.button("Reset Chat"):
        reset_chat()
        st.success("Chat history has been reset.")
    
    st.markdown("---")
    
    # PDF ì—…ë¡œë“œ ì„¹ì…˜ (ì„ íƒ ì‚¬í•­)
    st.header("Optional: Add PDF Document")
    st.write("Upload a PDF to enable document-based Q&A")
    
    # ëª¨ë°”ì¼ í™˜ê²½ì„ ìœ„í•œ ì„¤ëª… ì¶”ê°€
    uploaded_file = st.file_uploader("Choose your `.pdf` file (optional)", type="pdf")
    
    if uploaded_file:
        try:
            file_key = f"{session_id}-{uploaded_file.name}"

            with tempfile.TemporaryDirectory() as temp_dir:
                file_path = os.path.join(temp_dir, uploaded_file.name)
                
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())
                
                file_key = f"{session_id}-{uploaded_file.name}"
                st.write("Indexing your document...")
                
                # ì¸ë±ì‹± ê³¼ì •ì— ë¡œë”© ìƒíƒœ í‘œì‹œ
                with st.spinner("Processing document..."):
                    if file_key not in st.session_state.get('file_cache', {}):
                        if os.path.exists(temp_dir):
                            loader = PyPDFLoader(file_path)
                        else:    
                            st.error('Could not find the file you uploaded, please check again...')
                            st.stop()
                        
                        pages = loader.load_and_split()
                        
                        # ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
                        embeddings = get_embeddings_model()
                        if not embeddings:
                            st.error("ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                            st.stop()
                        
                        # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                        vectorstore = FAISS.from_documents(
                            documents=pages,
                            embedding=embeddings
                        )
                        
                        # ê²€ìƒ‰ê¸° ì„¤ì •
                        retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
                        
                        # LLM ì´ˆê¸°í™”
                        llm = initialize_basic_llm()
                        if not llm:
                            st.error("LLMì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            st.stop()
                        
                        # ì»¨í…ìŠ¤íŠ¸í™” í”„ë¡¬í”„íŠ¸ ì„¤ì •
                        contextualize_q_system_prompt = """ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
                        ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. 
                        ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."""

                        contextualize_q_prompt = ChatPromptTemplate.from_messages(
                            [
                                ("system", contextualize_q_system_prompt),
                                MessagesPlaceholder("chat_history"),
                                ("human", "{input}"),
                            ]
                        )

                        # ëŒ€í™” ê¸°ë¡ì„ ì¸ì‹í•˜ëŠ” ê²€ìƒ‰ê¸° ìƒì„±
                        history_aware_retriever = create_history_aware_retriever(
                            llm, retriever, contextualize_q_prompt
                        )

                        # ì§ˆë¬¸-ë‹µë³€ í”„ë¡¬í”„íŠ¸ ì„¤ì •
                        qa_system_prompt = """ë‹¹ì‹ ì€ ìœ ìš©í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì§€ì‹ì´ í’ë¶€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                        ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
                        
                        1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„¸í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
                        2. ë‹µë³€ì€ ìµœì†Œ 3-5ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ë©°, í•„ìš”í•œ ê²½ìš° ë” ìì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.
                        3. ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ì •ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
                        4. ë‹µë³€ ì‹œ í•µì‹¬ ê°œë…ì„ ë¨¼ì € ê°„ëµíˆ ì„¤ëª…í•œ í›„, ì„¸ë¶€ ë‚´ìš©ì„ ì œê³µí•˜ëŠ” êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”.
                        5. ê°€ëŠ¥í•œ ê²½ìš° ì˜ˆì‹œë‚˜ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€ì„ ê°•í™”í•˜ì„¸ìš”.
                        
                        ## ë‹µë³€ í˜•ì‹
                        ğŸ“ ë‹µë³€ ë‚´ìš©: (ìƒì„¸í•œ ë‹µë³€ì„ ì—¬ê¸°ì— ì‘ì„±)
                        
                        ğŸ“ ì°¸ê³  ìë£Œ: (ì‚¬ìš©í•œ ë¬¸ì„œì˜ ê´€ë ¨ ë¶€ë¶„)
                        
                        {context}"""
                        
                        qa_prompt = ChatPromptTemplate.from_messages(
                            [
                                ("system", qa_system_prompt),
                                MessagesPlaceholder("chat_history"),
                                ("human", "{input}"),
                            ]
                        )

                        # ë¬¸ì„œ ì²´ì¸ ìƒì„±
                        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

                        # ìµœì¢… RAG ì²´ì¸ ìƒì„±
                        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
                        
                        # ì„¸ì…˜ ìƒíƒœì— ì²´ì¸ ì €ì¥
                        st.session_state.rag_chain = rag_chain

                st.success("PDF loaded successfully! You can now ask questions about the document.")
                display_pdf(uploaded_file)
        except Exception as e:
            st.error(f"An error occurred: {e}")
            st.stop()     

# ê¸°ë³¸ LLM ì´ˆê¸°í™” (íŒŒì¼ ì—…ë¡œë“œ ì—†ì´ë„ ì‚¬ìš© ê°€ëŠ¥)
llm = initialize_basic_llm()
if not llm:
    st.warning("Grok API ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# ì›¹ì‚¬ì´íŠ¸ ì œëª©
st.title("Grok AI Chatbot")

# ëª¨ë“œ í‘œì‹œ
if "rag_chain" in st.session_state:
    st.info("ğŸ“„ Document Q&A mode: Ask questions about the uploaded PDF")
else:
    st.info("ğŸ’¬ General chat mode: Ask me anything!")

if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•´ ì…‹ì—…
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
# í”„ë¡¬í”„íŠ¸ ë¹„ìš©ì´ ë„ˆë¬´ ë§ì´ ì†Œìš”ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´
MAX_MESSAGES_BEFORE_DELETION = 4

# ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìœ ì €ì˜ ì¸í’‹ì„ ë°›ê³  ìœ„ì—ì„œ ë§Œë“  AI ì—ì´ì „íŠ¸ ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê¸°
if prompt := st.chat_input("Ask a question!"):
    # LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
    if not llm:
        st.error("Grok API ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    
    # ìœ ì €ê°€ ë³´ë‚¸ ì§ˆë¬¸ì´ë©´ ìœ ì € ì•„ì´ì½˜ê³¼ ì§ˆë¬¸ ë³´ì—¬ì£¼ê¸°
    # ë§Œì•½ í˜„ì¬ ì €ì¥ëœ ëŒ€í™” ë‚´ìš© ê¸°ë¡ì´ 4ê°œë³´ë‹¤ ë§ìœ¼ë©´ ìë¥´ê¸°
    if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
        # Remove the first two messages
        del st.session_state.messages[0]
        del st.session_state.messages[0]  
   
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AIê°€ ë³´ë‚¸ ë‹µë³€ì´ë©´ AI ì•„ì´ì½˜ì´ë‘ LLM ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê³  ìŠ¤íŠ¸ë¦¬ë°í•´ì„œ ë³´ì—¬ì£¼ê¸°
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            if "rag_chain" in st.session_state:
                # RAG ì²´ì¸ ì‚¬ìš© (PDF ì—…ë¡œë“œëœ ê²½ìš°)
                result = st.session_state.rag_chain.invoke({"input": prompt, "chat_history": st.session_state.messages})

                # ì¦ê±°ìë£Œ ë³´ì—¬ì£¼ê¸°
                with st.expander("Evidence context"):
                    st.write(result["context"])

                # ë‹µë³€ í‘œì‹œ (ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼)
                for chunk in result["answer"].split(" "):
                    full_response += chunk + " "
                    time.sleep(0.01)  # ìŠ¤íŠ¸ë¦¬ë° ì†ë„ ì¡°ì •
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
            else:
                # ê¸°ë³¸ LLM ì‚¬ìš© (PDF ì—…ë¡œë“œ ì—†ëŠ” ê²½ìš°)
                basic_llm = st.session_state.basic_llm
                
                # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
                basic_prompt = ChatPromptTemplate.from_messages([
                    ("system", "ë‹¹ì‹ ì€ ìœ ìš©í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì§€ì‹ì´ í’ë¶€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."),
                    MessagesPlaceholder("chat_history"),
                    ("human", "{input}")
                ])
                
                # ì±„íŒ… ì²´ì¸ ìƒì„±
                chat_history = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
                
                # Grok API í˜¸ì¶œ
                messages = [
                    SystemMessage(content="ë‹¹ì‹ ì€ ìœ ìš©í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì§€ì‹ì´ í’ë¶€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.")
                ]
                
                # ì´ì „ ëŒ€í™” ë‚´ìš© ì¶”ê°€
                for m in st.session_state.messages[:-1]:  # ë§ˆì§€ë§‰ ë©”ì‹œì§€(í˜„ì¬ ì§ˆë¬¸)ëŠ” ì œì™¸
                    if m["role"] == "user":
                        messages.append(HumanMessage(content=m["content"]))
                    elif m["role"] == "assistant":
                        messages.append(AIMessage(content=m["content"]))
                
                # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
                messages.append(HumanMessage(content=prompt))
                
                # API í˜¸ì¶œ
                response = basic_llm.invoke(messages)
                full_response = response.content
                
                # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜
                for i in range(0, len(full_response), 10):
                    chunk = full_response[i:i+10]
                    displayed_text = full_response[:i+10]
                    time.sleep(0.01)
                    message_placeholder.markdown(displayed_text + "â–Œ")
                message_placeholder.markdown(full_response)
                
        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            st.error(error_msg)
            full_response = error_msg
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})