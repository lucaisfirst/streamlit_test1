import os
import streamlit as st
import time
import base64
import uuid
import tempfile
import io
import numpy as np
from PIL import Image

# fitz ë¼ì´ë¸ŒëŸ¬ë¦¬(PyMuPDF) ê´€ë ¨ ì½”ë“œë¥¼ ì œê±°í•˜ê³  ê¸°ë³¸ PDF í‘œì‹œ ë°©ì‹ ì‚¬ìš©
try:
    from langchain_community.embeddings import OllamaEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_community.llms import Ollama
    from langchain_core.messages import HumanMessage, SystemMessage
    from langchain.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.chains.combine_documents import create_stuff_documents_chain
except ImportError as e:
    st.error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {str(e)}")
    st.warning("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
    st.code("pip install langchain langchain-community faiss-cpu", language="bash")
    st.stop()

if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()
    st.session_state.file_cache = {}

session_id = st.session_state.id
client = None

# Ollama ì„œë²„ URL ì„¤ì • (ê¸°ë³¸ê°’: localhost:11434)
OLLAMA_BASE_URL = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")

# Ollama ëª¨ë¸ ì„¤ì •
OLLAMA_EMBED_MODEL = os.environ.get("OLLAMA_EMBED_MODEL", "llama3.2")
OLLAMA_CHAT_MODEL = os.environ.get("OLLAMA_CHAT_MODEL", "llama3.2")

def reset_chat():
    st.session_state.messages = []
    st.session_state.context = None

def display_pdf(file):
    """PDF íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ iframeìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ í¬ì¸í„° ìœ„ì¹˜ ì´ˆê¸°í™”
        file.seek(0)
        
        # íŒŒì¼ ë°ì´í„°ë¥¼ ì½ì–´ì˜´
        file_data = file.read()
        
        # base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ iframeìœ¼ë¡œ í‘œì‹œ
        base64_pdf = base64.b64encode(file_data).decode('utf-8')
        pdf_display = f'''
            <iframe src="data:application/pdf;base64,{base64_pdf}" 
                    width="700"
                    height="800"
                    type="application/pdf">
            </iframe>
        '''
        st.markdown("### PDF ë¯¸ë¦¬ë³´ê¸°")
        st.markdown(pdf_display, unsafe_allow_html=True)
        
        # ì›ë³¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ
        file.seek(0)
        st.download_button(
            label="PDF ë‹¤ìš´ë¡œë“œ",
            data=file_data,
            file_name=file.name,
            mime="application/pdf"
        )
        
    except Exception as e:
        st.error(f"PDF í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    # íŒŒì¼ í¬ì¸í„° ìœ„ì¹˜ ë‹¤ì‹œ ì´ˆê¸°í™”
    file.seek(0)

with st.sidebar:
    st.header(f"Add your documents!")
    
    uploaded_file = st.file_uploader("Choose your `.pdf` file", type="pdf")

    if uploaded_file:
        try:
            file_key = f"{session_id}-{uploaded_file.name}"

            with tempfile.TemporaryDirectory() as temp_dir:
                file_path = os.path.join(temp_dir, uploaded_file.name)
                
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())
                
                file_key = f"{session_id}-{uploaded_file.name}"
                st.write("Indexing your document...")

                if file_key not in st.session_state.get('file_cache', {}):
                    if os.path.exists(temp_dir):
                        loader = PyPDFLoader(file_path)
                    else:    
                        st.error('Could not find the file you uploaded, please check again...')
                        st.stop()
                    
                    pages = loader.load_and_split()
                    
                    # Ollama ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
                    embeddings = OllamaEmbeddings(
                        base_url=OLLAMA_BASE_URL,
                        model=OLLAMA_EMBED_MODEL
                    )
                    
                    # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± (Chroma ëŒ€ì‹  ì‚¬ìš©)
                    vectorstore = FAISS.from_documents(
                        documents=pages,
                        embedding=embeddings
                    )
                    
                    # ê²€ìƒ‰ê¸° ì„¤ì •
                    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
                    
                    # Ollama LLM ì„¤ì • (ë§¤ê°œë³€ìˆ˜ ì¶”ê°€)
                    llm = Ollama(
                        base_url=OLLAMA_BASE_URL,
                        model=OLLAMA_CHAT_MODEL,
                        temperature=0.7,  # ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš©
                        num_predict=512,  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ ì¦ê°€
                        stop=["<|im_end|>"],  # ì ì ˆí•œ ì¤‘ë‹¨ í† í° ì„¤ì •
                        repeat_penalty=1.1,  # ë°˜ë³µ ë°©ì§€
                        top_k=40,  # ë‹¤ì–‘í•œ ë‹¨ì–´ ì„ íƒ í—ˆìš©
                        top_p=0.9  # ë‹¤ì–‘ì„± í™•ë³´
                    )
                    
                    # ì»¨í…ìŠ¤íŠ¸í™” í”„ë¡¬í”„íŠ¸ ì„¤ì •
                    contextualize_q_system_prompt = """ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
                    ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. 
                    ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."""

                    contextualize_q_prompt = ChatPromptTemplate.from_messages(
                        [
                            ("system", contextualize_q_system_prompt),
                            MessagesPlaceholder("chat_history"),
                            ("human", "{input}"),
                        ]
                    )

                    # ëŒ€í™” ê¸°ë¡ì„ ì¸ì‹í•˜ëŠ” ê²€ìƒ‰ê¸° ìƒì„±
                    history_aware_retriever = create_history_aware_retriever(
                        llm, retriever, contextualize_q_prompt
                    )

                    # ì§ˆë¬¸-ë‹µë³€ í”„ë¡¬í”„íŠ¸ ì„¤ì •
                    qa_system_prompt = """ë‹¹ì‹ ì€ ìœ ìš©í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì§€ì‹ì´ í’ë¶€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                    ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
                    
                    1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„¸í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
                    2. ë‹µë³€ì€ ìµœì†Œ 3-5ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ë©°, í•„ìš”í•œ ê²½ìš° ë” ìì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.
                    3. ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ì •ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
                    4. ë‹µë³€ ì‹œ í•µì‹¬ ê°œë…ì„ ë¨¼ì € ê°„ëµíˆ ì„¤ëª…í•œ í›„, ì„¸ë¶€ ë‚´ìš©ì„ ì œê³µí•˜ëŠ” êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”.
                    5. ê°€ëŠ¥í•œ ê²½ìš° ì˜ˆì‹œë‚˜ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€ì„ ê°•í™”í•˜ì„¸ìš”.
                    
                    ## ë‹µë³€ í˜•ì‹
                    ğŸ“ ë‹µë³€ ë‚´ìš©: (ìƒì„¸í•œ ë‹µë³€ì„ ì—¬ê¸°ì— ì‘ì„±)
                    
                    ğŸ“ ì°¸ê³  ìë£Œ: (ì‚¬ìš©í•œ ë¬¸ì„œì˜ ê´€ë ¨ ë¶€ë¶„)
                    
                    {context}"""
                    
                    qa_prompt = ChatPromptTemplate.from_messages(
                        [
                            ("system", qa_system_prompt),
                            MessagesPlaceholder("chat_history"),
                            ("human", "{input}"),
                        ]
                    )

                    # ë¬¸ì„œ ì²´ì¸ ìƒì„±
                    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

                    # ìµœì¢… RAG ì²´ì¸ ìƒì„±
                    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ì²´ì¸ ì €ì¥
                    st.session_state.rag_chain = rag_chain

                st.success("Ready to Chat!")
                display_pdf(uploaded_file)
        except Exception as e:
            st.error(f"An error occurred: {e}")
            st.stop()     

# ì›¹ì‚¬ì´íŠ¸ ì œëª©
st.title("Llama 3.2 LLM Chatbot")

if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•´ ì…‹ì—…
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
# í”„ë¡¬í”„íŠ¸ ë¹„ìš©ì´ ë„ˆë¬´ ë§ì´ ì†Œìš”ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´
MAX_MESSAGES_BEFORE_DELETION = 4

# ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìœ ì €ì˜ ì¸í’‹ì„ ë°›ê³  ìœ„ì—ì„œ ë§Œë“  AI ì—ì´ì „íŠ¸ ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê¸°
if prompt := st.chat_input("Ask a question!"):
    
    # ìœ ì €ê°€ ë³´ë‚¸ ì§ˆë¬¸ì´ë©´ ìœ ì € ì•„ì´ì½˜ê³¼ ì§ˆë¬¸ ë³´ì—¬ì£¼ê¸°
    # ë§Œì•½ í˜„ì¬ ì €ì¥ëœ ëŒ€í™” ë‚´ìš© ê¸°ë¡ì´ 4ê°œë³´ë‹¤ ë§ìœ¼ë©´ ìë¥´ê¸°
    if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
        # Remove the first two messages
        del st.session_state.messages[0]
        del st.session_state.messages[0]  
   
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AIê°€ ë³´ë‚¸ ë‹µë³€ì´ë©´ AI ì•„ì´ì½˜ì´ë‘ LLM ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê³  ìŠ¤íŠ¸ë¦¬ë°í•´ì„œ ë³´ì—¬ì£¼ê¸°
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        if "rag_chain" in st.session_state:
            # RAG ì²´ì¸ ì‚¬ìš©
            result = st.session_state.rag_chain.invoke({"input": prompt, "chat_history": st.session_state.messages})

            # ì¦ê±°ìë£Œ ë³´ì—¬ì£¼ê¸°
            with st.expander("Evidence context"):
                st.write(result["context"])

            # ë‹µë³€ í‘œì‹œ (ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼)
            for chunk in result["answer"].split(" "):
                full_response += chunk + " "
                time.sleep(0.05)  # ìŠ¤íŠ¸ë¦¬ë° ì†ë„ ì¡°ì • (Ollamaê°€ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ)
                message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
        else:
            # ë¬¸ì„œê°€ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
            full_response = "ë¨¼ì € PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            message_placeholder.markdown(full_response)
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})