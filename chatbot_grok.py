import os
import streamlit as st
import time
import base64
import uuid
import tempfile
import io
import numpy as np
from PIL import Image
from langchain.llms import OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.chains.combine_documents import create_stuff_documents_chain

# í˜ì´ì§€ ì„¤ì • - ëª¨ë°”ì¼ í˜¸í™˜ì„± ê°œì„ 
st.set_page_config(
    page_title="PDF Chatbot",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë°˜ì‘í˜• ë””ìì¸ë§Œ ìœ ì§€í•˜ê³  ê¸°ì¡´ ìƒ‰ìƒ ë³µì›
st.markdown("""
<style>
    /* ëª¨ë°”ì¼ í™˜ê²½ì—ì„œì˜ ìŠ¤íƒ€ì¼ ì¡°ì • */
    @media (max-width: 768px) {
        .main .block-container {
            padding-top: 1rem;
            padding-left: 0.5rem;
            padding-right: 0.5rem;
        }
        .stSidebar {
            width: 100% !important;
        }
        iframe {
            width: 100% !important;
            height: 400px !important;
        }
        .pdf-container {
            width: 100% !important;
        }
    }
</style>
""", unsafe_allow_html=True)

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í™•ì¸
try:
    from langchain_community.embeddings import OpenAIEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_community.document_loaders import PyPDFLoader
    from langchain.llms import OpenAI
    from langchain_core.messages import HumanMessage, SystemMessage
    from langchain.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.chains.combine_documents import create_stuff_documents_chain
except ImportError as e:
    st.error(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {str(e)}")
    st.warning("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
    st.code("pip install langchain langchain-community openai pypdf streamlit", language="bash")
    st.stop()

if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()
    st.session_state.file_cache = {}

session_id = st.session_state.id
client = None

# OpenAI API í‚¤ ì„¤ì •
openai_api_key = os.environ.get("OPENAI_API_KEY", "")
if not openai_api_key:
    openai_api_key = st.sidebar.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if not openai_api_key:
        st.warning("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()
os.environ["OPENAI_API_KEY"] = openai_api_key

# ëª¨ë¸ ì„¤ì •
EMBED_MODEL = "text-embedding-ada-002"
CHAT_MODEL = "gpt-3.5-turbo"

def reset_chat():
    st.session_state.messages = []
    st.session_state.context = None

def display_pdf(file):
    """PDF íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ iframeìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ í¬ì¸í„° ìœ„ì¹˜ ì´ˆê¸°í™”
        file.seek(0)
        
        # íŒŒì¼ ë°ì´í„°ë¥¼ ì½ì–´ì˜´
        file_data = file.read()
        
        # base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ iframeìœ¼ë¡œ í‘œì‹œ
        base64_pdf = base64.b64encode(file_data).decode('utf-8')
        
        # ë°˜ì‘í˜• ë””ìì¸ ìœ ì§€í•˜ë©´ì„œ ê¸°ì¡´ ìŠ¤íƒ€ì¼ë¡œ ë³µì›
        pdf_display = f'''
            <div class="pdf-container">
                <iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height="600px" type="application/pdf"></iframe>
            </div>
        '''
        st.markdown(pdf_display, unsafe_allow_html=True)
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€
        st.download_button(
            label="PDF ë‹¤ìš´ë¡œë“œ",
            data=file,
            file_name="document.pdf",
            mime="application/pdf"
        )
    except Exception as e:
        st.error(f"PDF í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

def initialize_basic_llm():
    if "basic_llm" not in st.session_state:
        st.session_state.basic_llm = OpenAI(
            model=CHAT_MODEL,
            temperature=0.7,
            max_tokens=512,
            stop=["<|im_end|>"],
            repeat_penalty=1.1,
        )
    return st.session_state.basic_llm

def main():
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "context" not in st.session_state:
        st.session_state.context = None
    
    if "uploaded_file" not in st.session_state:
        st.session_state.uploaded_file = None
    
    if "file_processed" not in st.session_state:
        st.session_state.file_processed = False
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.title("Chatbot ì˜µì…˜")
        
        # ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ì±„íŒ… ì´ˆê¸°í™”"):
            reset_chat()
        
        st.header("PDF ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)")
        uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")
        
        # íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆê³  ì´ì „ì— ì²˜ë¦¬ëœ íŒŒì¼ê³¼ ë‹¤ë¥¸ ê²½ìš°
        if uploaded_file is not None and (st.session_state.uploaded_file is None or 
                                         uploaded_file.name != st.session_state.uploaded_file.name):
            st.session_state.uploaded_file = uploaded_file
            st.session_state.file_processed = False
            reset_chat()  # ìƒˆ íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì±„íŒ… ì´ˆê¸°í™”
            
            with st.spinner("PDF íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_path = tmp_file.name
                    
                    # PDF ë¡œë” ì‚¬ìš©
                    loader = PyPDFLoader(tmp_path)
                    documents = loader.load()
                    
                    # í…ìŠ¤íŠ¸ ë¶„í• 
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1000,
                        chunk_overlap=200
                    )
                    splits = text_splitter.split_documents(documents)
                    
                    # ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                    if len(splits) > 0:
                        try:
                            # OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
                            embeddings = OpenAIEmbeddings(
                                model=EMBED_MODEL
                            )
                            
                            # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                            vectorstore = FAISS.from_documents(
                                documents=splits,
                                embedding=embeddings
                            )
                            
                            # ê²€ìƒ‰ê¸° ìƒì„±
                            retriever = vectorstore.as_retriever(
                                search_type="similarity",
                                search_kwargs={"k": 4}
                            )
                            
                            # OpenAI LLM ì„¤ì • (ë§¤ê°œë³€ìˆ˜ ì¶”ê°€)
                            llm = OpenAI(
                                model=CHAT_MODEL,
                                temperature=0.7,  # ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš©
                                max_tokens=512,  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ ì¦ê°€
                                stop=["<|im_end|>"],  # ì ì ˆí•œ ì¤‘ë‹¨ í† í° ì„¤ì •
                                repeat_penalty=1.1,  # ë°˜ë³µ ë°©ì§€
                            )
                            
                            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
                            prompt = ChatPromptTemplate.from_template("""
                            <context>
                            {context}
                            </context>
                            
                            ì§ˆë¬¸ì— ëŒ€í•´ ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”. 
                            ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, "ì œê³µëœ ë¬¸ì„œì—ì„œ ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë§í•˜ì„¸ìš”.
                            ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì œê³µí•˜ê³ , ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                            
                            ì§ˆë¬¸: {question}
                            """)
                            
                            # ë¬¸ì„œ ì²´ì¸ ìƒì„±
                            document_chain = create_stuff_documents_chain(llm, prompt)
                            
                            # ê²€ìƒ‰ ì²´ì¸ ìƒì„±
                            retrieval_chain = create_retrieval_chain(retriever, document_chain)
                            
                            # ì»¨í…ìŠ¤íŠ¸ ì €ì¥
                            st.session_state.context = retrieval_chain
                            st.session_state.file_processed = True
                            
                            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                            os.unlink(tmp_path)
                            
                            st.success("PDFê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        except Exception as e:
                            st.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    else:
                        st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    # ë©”ì¸ ì˜ì—­
    st.title("PDF Chatbot")
    
    # ê¸°ë³¸ LLM ì´ˆê¸°í™” (íŒŒì¼ ì—†ì´ë„ ì‘ë™)
    initialize_basic_llm()
    
    # ì—…ë¡œë“œëœ íŒŒì¼ì´ ìˆìœ¼ë©´ í‘œì‹œ
    if st.session_state.uploaded_file is not None:
        st.subheader("ì—…ë¡œë“œëœ PDF")
        display_pdf(st.session_state.uploaded_file)
    else:
        st.info("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŒŒì¼ ì—…ë¡œë“œëŠ” ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤.")
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.subheader("ì±„íŒ…")
    
    # ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            try:
                # íŒŒì¼ì´ ì²˜ë¦¬ë˜ì—ˆìœ¼ë©´ RAG ì‚¬ìš©, ì•„ë‹ˆë©´ ê¸°ë³¸ LLM ì‚¬ìš©
                if st.session_state.context and st.session_state.file_processed:
                    # RAGë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
                    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                        response = st.session_state.context.invoke({
                            "question": prompt
                        })
                        full_response = response["answer"]
                else:
                    # ê¸°ë³¸ LLMì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
                    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                        system_prompt = """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
                        ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. 
                        ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì œê³µí•˜ê³ , ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
                        
                        prompt_template = ChatPromptTemplate.from_messages([
                            ("system", system_prompt),
                            ("human", "{question}")
                        ])
                        
                        chain = prompt_template | st.session_state.basic_llm | StrOutputParser()
                        full_response = chain.invoke({"question": prompt})
                
                # íƒ€ì´í•‘ íš¨ê³¼
                for chunk in full_response.split():
                    full_response_so_far = full_response[:full_response.find(chunk) + len(chunk)]
                    message_placeholder.markdown(full_response_so_far + "â–Œ")
                    time.sleep(0.01)
                
                message_placeholder.markdown(full_response)
            except Exception as e:
                full_response = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                message_placeholder.markdown(full_response)
            
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main() 